
import os
import time
import argparse
from pathlib import Path
import sys
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import deepspeed


from model_moe_small import MoETransformer

class StreamConcatDataset(Dataset):
    def __init__(self, token_ids, block_size=512):
        self.block_size = block_size
        self.ids = token_ids
        self.num_chunks = max(1, len(self.ids) // block_size)
    def __len__(self):
        return self.num_chunks
    def __getitem__(self, idx):
        s = idx * self.block_size
        chunk = self.ids[s:s + self.block_size]
        if len(chunk) < self.block_size:
            chunk = chunk + [tokenizer.eos_token_id] * (self.block_size - len(chunk))
        x = torch.tensor(chunk[:-1], dtype=torch.long)
        y = torch.tensor(chunk[1:], dtype=torch.long)
        return x, y

def collate_fn(batch):
    xs = torch.stack([b[0] for b in batch])
    ys = torch.stack([b[1] for b in batch])
    return xs, ys

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--coding", required=True)
    parser.add_argument("--stories", required=True)
    parser.add_argument("--rawweb", required=True)
    parser.add_argument("--qa", required=True)
    parser.add_argument("--deepspeed_config", default="deepspeed_config.json")
    parser.add_argument("--output_dir", default="./moe_small_ckpt")
    parser.add_argument("--vocab_size", type=int, default=32000)
    parser.add_argument("--block_size", type=int, default=512)
    parser.add_argument("--max_steps", type=int, default=200000)
    parser.add_argument("--limit_time_hours", type=float, default=5.0)
    # accept unknown args (DeepSpeed adds --local_rank etc.)
    args, _ = parser.parse_known_args()

    files = [args.coding, args.stories, args.rawweb, args.qa]
    # load pretrained tokenizer (gpt2 byte-level bpe)
    print("Loading pretrained tokenizer (gpt2)...")
    global tokenizer
    tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": "<pad>"})

    # build token stream (cap tokens to keep memory modest)
    token_ids = []
    max_tokens_in_mem = 6_000_000
    for f in files:
        if not os.path.exists(f):
            print("Warning: missing file", f)
            continue
        with open(f, "r", encoding="utf-8", errors="ignore") as fh:
            for line in fh:
                s = line.strip()
                if not s:
                    continue
                enc = tokenizer.encode(s)
                # AutoTokenizer.encode may return list; ensure list of ids
                if hasattr(enc, "ids"):
                    enc_ids = enc.ids
                elif isinstance(enc, list):
                    enc_ids = enc
                else:
                    enc_ids = tokenizer(s, add_special_tokens=False)["input_ids"]
                token_ids.extend(enc_ids + [tokenizer.eos_token_id])
                if len(token_ids) >= max_tokens_in_mem:
                    break
        if len(token_ids) >= max_tokens_in_mem:
            break

    if len(token_ids) == 0:
        print("No tokens found in your dataset files. Exiting.")
        sys.exit(1)

    print(f"Collected ~{len(token_ids)} tokens (capped). Building dataset...")
    dataset = StreamConcatDataset(token_ids, block_size=args.block_size)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn, drop_last=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = MoETransformer(vocab_size=args.vocab_size)
    model.train()

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.01)

    ds_args = argparse.Namespace(**{"deepspeed_config": args.deepspeed_config})
    engine, optimizer, _, _ = deepspeed.initialize(args=ds_args, model=model, optimizer=optimizer, model_parameters=model.parameters())

    start_time = time.time()
    max_seconds = args.limit_time_hours * 3600.0
    step = 0

    print("Starting training loop. Wall-clock limit (s):", max_seconds)
    try:
        while step < args.max_steps:
            for xb, yb in dataloader:
                xb = xb.to(engine.local_rank if hasattr(engine, "local_rank") else device)
                yb = yb.to(engine.local_rank if hasattr(engine, "local_rank") else device)

                logits = engine(xb)
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))
                engine.backward(loss)
                engine.step()

                step += 1
                if step % 20 == 0:
                    elapsed = time.time() - start_time
                    print(f"step {step} loss {loss.item():.4f} elapsed {elapsed/60:.2f} min")

                if step % 1000 == 0:
                    ckpt = Path(args.output_dir) / f"step_{step}"
                    ckpt.mkdir(parents=True, exist_ok=True)
                    engine.save_checkpoint(str(ckpt))
                    print("Saved checkpoint:", ckpt)

                if time.time() - start_time > max_seconds:
                    print("Reached wall-clock limit. Saving checkpoint and exiting.")
                    engine.save_checkpoint(args.output_dir)
                    sys.exit(0)

                if step >= args.max_steps:
                    break
    except KeyboardInterrupt:
        print("KeyboardInterrupt â€” saving checkpoint and exiting.")
        engine.save_checkpoint(args.output_dir)
        sys.exit(0)

    engine.save_checkpoint(args.output_dir)
    print("Training finished. Checkpoint saved to", args.output_dir)

if __name__ == "__main__":
    main()
